{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8jKiDIBAima"
      },
      "source": [
        "# Cover Type Classification\n",
        "\n",
        "## Objective: Build a deep learning model to predict the forest cover type from different cartographic variables.\n",
        "\n",
        "### Given\n",
        "    1. Cover Types:\n",
        "      - Spruce/Fir\n",
        "      - Lodgepole Pine\n",
        "      - Ponderosa Pine\n",
        "      - Cottonwood/Willow\n",
        "      - Aspen\n",
        "      - Douglas-fir\n",
        "      - Krummholz\n",
        "\n",
        "    2. Dataset\n",
        "      - File: `cover_data.csv`\n",
        "      - Contains 581,012 observations\n",
        "      - Each observation has 55 columns (54 features and the last one being the class)\n",
        "\n",
        "### Assumptions\n",
        "    1. Test Data: No separate test dataset provided, so a portion of the input must be held out as test data.\n",
        "    2. Prediction Focus: No information on whether to prioritize precision or recall, so both should be maximized.\n",
        "\n",
        "### Expected Output\n",
        "    1. A high-performing deep learning model.\n",
        "    2. Performance metrics over epochs (accuracy and loss plots).\n",
        "    3. Classification metrics:\n",
        "      - Heatmap of the confusion matrix\n",
        "      - Classification report\n",
        "    4. Conclusions, thoughts, and suggestions for improving classification accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFcHVSdfAimg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sweetviz as sv\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0IUzsDLAiml"
      },
      "outputs": [],
      "source": [
        "# Disable those annoying warnings\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Turn off GPU usage for tf\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocaInuvfAimn"
      },
      "source": [
        "### Define some helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DcZ49LXAimo"
      },
      "outputs": [],
      "source": [
        "def prep_data(raw_df):\n",
        "    \"\"\"\n",
        "    Prepare data that can be readily consumed by ML/DL algorithms.\n",
        "    - separate features from class variables\n",
        "    - split into training and testing dataset\n",
        "    - scale numerical data\n",
        "\n",
        "    param: a dataframe of input data\n",
        "    output: X_train_normalized, X_test_normalized, y_train, y_test\n",
        "    \"\"\"\n",
        "    raw_data = raw_df.values\n",
        "    X, y = raw_data[:, :-1], raw_data[:, -1]\n",
        "\n",
        "    # Split into train and test set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
        "\n",
        "    # normalize data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_normalized = scaler.fit_transform(X_train)\n",
        "    X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "    return X_train_normalized, X_test_normalized, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCBD9-DSAimp"
      },
      "outputs": [],
      "source": [
        "def build_model(num_features):\n",
        "    \"\"\"\n",
        "    Build the model architecture (and compile it).\n",
        "    input: number of features\n",
        "    output: Keras model object.\n",
        "    \"\"\"\n",
        "    classifier = keras.Sequential()\n",
        "    classifier.add(layers.Dense(64, input_dim=num_features, activation='relu'))\n",
        "    #classifier.add(layers.Dropout(0.3))\n",
        "    classifier.add(layers.Dense(32, activation='relu'))\n",
        "    #classifier.add(layers.Dropout(0.3))\n",
        "    classifier.add(layers.Dense(8, activation='softmax'))\n",
        "    classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0SxWXGHAimv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}